{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a750deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "from scipy import stats, interpolate\n",
    "import os\n",
    "from tensorflow.keras import backend as K  \n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "356a026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load (basedir, name, model=None):\n",
    "\n",
    "    # Import(s)\n",
    "    from keras.models import load_model\n",
    "\n",
    "    # Load full pre-trained model or model weights\n",
    "    if model is None:\n",
    "        model = load_model(basedir + '{}.h5'.format(name))\n",
    "    else:\n",
    "        model.load_weights(basedir + '{}_weights.h5'.format(name))\n",
    "        pass\n",
    "\n",
    "    # Load associated training histories\n",
    "    try:\n",
    "        history_file = basedir + 'history__{}.json'.format(name)\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "            pass\n",
    "    except:\n",
    "        print (\"[WARN] Could not find history file {}.\") #\n",
    "        history = None\n",
    "        pass\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24555ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data (path, name='dataset', train=None, test=None, signal=None, background=None, sample=None, seed=21, replace=True):\n",
    "    \"\"\"\n",
    "    General script to load data, common to all run scripts.\n",
    "\n",
    "    Arguments:\n",
    "        path: The path to the HDF5 file, from which data should be loaded.\n",
    "        name: Name of the dataset, as stored in the HDF5 file.\n",
    "        ...\n",
    "\n",
    "    Returns:\n",
    "        Tuple of pandas.DataFrame containing the loaded; list of loaded features\n",
    "        to be used for training; and list of features to be used for mass-\n",
    "        decorrelation.\n",
    "\n",
    "    Raises:\n",
    "        IOError: If no HDF5 file exists at the specified `path`.\n",
    "        KeyError: If the HDF5 does not contained a dataset named `name`.\n",
    "        KeyError: If any of the necessary features are not present in the loaded\n",
    "            dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check(s)\n",
    "    #assert False not in [train, test, signal, background]\n",
    "    #if sample: assert 0 < sample and sample < 1.\n",
    "\n",
    "    # Read data from HDF5 file\n",
    "    data = pd.read_hdf(path)\n",
    "\n",
    "    # Subsample signal by x10 for testing: 1E+07 -> 1E+06\n",
    "    np.random.seed(7)\n",
    "    try:\n",
    "        msk_test  = data['train'] == 0\n",
    "        msk_train = ~msk_test\n",
    "        msk_bkg = data['signal'] == 0\n",
    "        msk_sig = ~msk_bkg\n",
    "        #idx_sig = np.where(msk_sig)[0]\n",
    "        #idx_sig = np.random.choice(idx_sig, int(msk_sig.sum() * 0.1), replace=False)\n",
    "        #msk_sig = np.zeros_like(msk_bkg).astype(bool)\n",
    "        #msk_sig[idx_sig] = True\n",
    "        #data = data[msk_train | (msk_test & (msk_sig | msk_bkg))]\n",
    "    except:\n",
    "        log.warning(\"Some of the keys ['train', 'signal'] were not present in file {}\".format(path))\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    # Define feature collections to use\n",
    "    features_input = ['Tau21pJ1', 'Tau21J2', 'Tau32J1', 'Tau32J2']\n",
    "    features_decorrelation = ['m']\n",
    "\n",
    "    # Split data\n",
    "    if train:\n",
    "        log.info(\"load_data: Selecting only training data.\")\n",
    "        data = data[data['train']  == 1]\n",
    "        pass\n",
    "\n",
    "    if test:\n",
    "        # log.info(\"load_data: Selecting only testing data.\")\n",
    "        data = data[data['train']  == 0]\n",
    "        pass\n",
    "\n",
    "    if signal:\n",
    "        log.info(\"load_data: Selecting only signal data.\")\n",
    "        data = data[data['signal'] == 1]\n",
    "        pass\n",
    "\n",
    "    if background:\n",
    "        log.info(\"load_data: Selecting only background data.\")\n",
    "        data = data[data['signal'] == 0]\n",
    "        pass\n",
    "\n",
    "    if sample:\n",
    "        log.info(\"load_data: Selecting a random fraction {:.2f} of data (replace = {}, seed = {}).\".format(sample, replace, seed))\n",
    "        data = data.sample(frac=sample, random_state=seed, replace=False)\n",
    "        pass\n",
    "\n",
    "    # Return\n",
    "    return data, features_input, features_decorrelation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59f174f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "def load_dataT21(input_frame):\n",
    "    dataset = input_frame[[\"mj1\",\"tau1j1\",\"tau2j1\",\"mj2\",\"tau1j2\",\"tau2j2\"]]\n",
    "    dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
    "    dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
    "    dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
    "\n",
    "    dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
    "    dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
    "    dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau21jOne\"] = dataset[\"tau2jOne\"]/dataset[\"tau1jOne\"]\n",
    "    dataset[\"tau21jTwo\"] = dataset[\"tau2jTwo\"]/dataset[\"tau1jTwo\"]\n",
    "    dataset[\"mjTwo\"] = dataset[\"mjTwo\"]/1000. + 2*alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjOne\"] = dataset[\"mjOne\"]/1000. + alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjDelta\"] = (dataset[\"mjTwo\"] - dataset[\"mjOne\"])\n",
    "\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = dataset[[\"mjj\",\"mjOne\",\"mjDelta\", \"tau21jOne\", \"tau21jTwo\"]]\n",
    "    return dataset.to_numpy()\n",
    "\n",
    "def load_dataT32(input_frame):\n",
    "    dataset = input_frame[[\"mj1\",\"tau1j1\",\"tau2j1\",\"mj2\",\"tau1j2\",\"tau2j2\",\"tau3j1\",\"tau3j2\"]]\n",
    "    dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
    "    dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
    "    dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
    "\n",
    "    dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau3jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
    "    dataset[\"tau3jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
    "    dataset[\"tau32jOne\"] = dataset[\"tau3jOne\"]/dataset[\"tau2jOne\"]\n",
    "    dataset[\"tau32jTwo\"] = dataset[\"tau3jTwo\"]/dataset[\"tau2jTwo\"] \n",
    "    dataset[\"mjTwo\"] = dataset[\"mjTwo\"]/1000. + 2*alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjOne\"] = dataset[\"mjOne\"]/1000. + alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjDelta\"] = (dataset[\"mjTwo\"] - dataset[\"mjOne\"])\n",
    "\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = dataset[[\"mjj\",\"mjOne\",\"mjDelta\", \"tau32jOne\", \"tau32jTwo\"]]\n",
    "    return dataset.to_numpy()\n",
    "\n",
    "def load_data_more(input_frame):\n",
    "    dataset = input_frame[[\"mj1\",\"tau1j1\",\"tau2j1\",\"mj2\",\"tau1j2\",\"tau2j2\",\"tau3j1\",\"tau3j2\"]]\n",
    "    \n",
    "    #Masa ya al cuadrado\n",
    "    dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
    "    \n",
    "    \n",
    "    dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
    "    dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
    "\n",
    "    dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
    "    dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
    "    dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau21jOne\"] = dataset[\"tau2jOne\"]/dataset[\"tau1jOne\"]\n",
    "    dataset[\"tau21jTwo\"] = dataset[\"tau2jTwo\"]/dataset[\"tau1jTwo\"]\n",
    "    \n",
    "    #tau32\n",
    "    dataset[\"tau3jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
    "    dataset[\"tau3jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
    "    dataset[\"tau32jOne\"] = dataset[\"tau3jOne\"]/dataset[\"tau2jOne\"]\n",
    "    dataset[\"tau32jTwo\"] = dataset[\"tau3jTwo\"]/dataset[\"tau2jTwo\"]   \n",
    "    \n",
    "    #pT\n",
    "    dataset[\"pT1\"] = (input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2)**0.5/1000.\n",
    "    dataset[\"pT2\"] = (input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2)**0.5/1000.\n",
    "    dataset[\"pTjOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"pT1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"pT2\"]\n",
    "    dataset[\"pTjTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"pT1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"pT2\"]    \n",
    "    \n",
    "    #eta\n",
    "    dataset[\"E1\"] = ((1000*dataset[\"pT1\"])**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5/1000.\n",
    "    dataset[\"E2\"] = ((1000*dataset[\"pT2\"])**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5/1000.\n",
    "    dataset[\"y1\"] = 0.5*np.log((dataset[\"E1\"] + input_frame[\"pzj1\"]/1000.)/(dataset[\"E1\"] - input_frame[\"pzj1\"]/1000.))\n",
    "    dataset[\"y2\"] = 0.5*np.log((dataset[\"E2\"] + input_frame[\"pzj2\"]/1000.)/(dataset[\"E2\"] - input_frame[\"pzj2\"]/1000.))\n",
    "    dataset[\"Dy\"] = dataset[\"y1\"]-dataset[\"y2\"]\n",
    "    \n",
    "    dataset[\"mjTwo\"] = dataset[\"mjTwo\"]/1000. + 2*alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjOne\"] = dataset[\"mjOne\"]/1000. + alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjDelta\"] = (dataset[\"mjTwo\"] - dataset[\"mjOne\"])\n",
    "\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = dataset[[\"mjj\",\"mjOne\",\"mjDelta\", \"tau21jOne\", \"tau21jTwo\", \"tau32jOne\", \"tau32jTwo\",\"pTjOne\",\"pTjTwo\",\"Dy\"]]\n",
    "    return dataset.to_numpy()\n",
    "\n",
    "column_labelsT21 = [\n",
    "    r\"$m_{JJ}$\",\n",
    "    r\"$m^{J_1}$\",\n",
    "    \"$m^{J_2} - m^{J_1}$\",\n",
    "    r\"$\\tau_{2,1}^{J_1}$\", \n",
    "    r\"$\\tau_{2,1}^{J_2}$\",\n",
    "]\n",
    "\n",
    "column_labelsT32 = [\n",
    "    r\"$m_{JJ}$\",\n",
    "    r\"$m^{J_1}$\",\n",
    "    \"$m^{J_2} - m^{J_1}$\",\n",
    "    r\"$\\tau_{3,2}^{J_1}$\", \n",
    "    r\"$\\tau_{3,2}^{J_2}$\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e14fdc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_data=pd.read_hdf(\"C:/Users/knukl/Desktop/FINAL/input/events_anomalydetection_v2.features.h5\")\n",
    "original_data=pd.read_hdf(\"/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/content/events_anomalydetection_v2.features.h5\")\n",
    "features_sig=original_data.query(\"label == 1\")\n",
    "features_bg=original_data.query(\"label == 0\")\n",
    "\n",
    "\n",
    "features_bg2=pd.read_hdf(\"/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/content/events_anomalydetection_DelphesHerwig_qcd_features.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0f8780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pxj1</th>\n",
       "      <th>pyj1</th>\n",
       "      <th>pzj1</th>\n",
       "      <th>mj1</th>\n",
       "      <th>tau1j1</th>\n",
       "      <th>tau2j1</th>\n",
       "      <th>tau3j1</th>\n",
       "      <th>pxj2</th>\n",
       "      <th>pyj2</th>\n",
       "      <th>pzj2</th>\n",
       "      <th>mj2</th>\n",
       "      <th>tau1j2</th>\n",
       "      <th>tau2j2</th>\n",
       "      <th>tau3j2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1467.239990</td>\n",
       "      <td>611.502014</td>\n",
       "      <td>511.101990</td>\n",
       "      <td>38.896000</td>\n",
       "      <td>8.290660</td>\n",
       "      <td>4.836080</td>\n",
       "      <td>4.260190</td>\n",
       "      <td>1403.579956</td>\n",
       "      <td>-674.551025</td>\n",
       "      <td>-451.670990</td>\n",
       "      <td>237.893997</td>\n",
       "      <td>79.815102</td>\n",
       "      <td>21.010300</td>\n",
       "      <td>16.757601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1211.239990</td>\n",
       "      <td>347.315002</td>\n",
       "      <td>547.963013</td>\n",
       "      <td>389.532013</td>\n",
       "      <td>191.804001</td>\n",
       "      <td>99.562798</td>\n",
       "      <td>70.872200</td>\n",
       "      <td>619.341003</td>\n",
       "      <td>-62.177299</td>\n",
       "      <td>-1944.040039</td>\n",
       "      <td>22.999201</td>\n",
       "      <td>8.042190</td>\n",
       "      <td>6.335090</td>\n",
       "      <td>5.525360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1229.619995</td>\n",
       "      <td>649.857971</td>\n",
       "      <td>8.089170</td>\n",
       "      <td>72.155502</td>\n",
       "      <td>47.168098</td>\n",
       "      <td>37.243198</td>\n",
       "      <td>33.658199</td>\n",
       "      <td>1196.250000</td>\n",
       "      <td>-647.896973</td>\n",
       "      <td>-1283.109985</td>\n",
       "      <td>78.230698</td>\n",
       "      <td>15.292900</td>\n",
       "      <td>13.944200</td>\n",
       "      <td>10.013500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-693.304016</td>\n",
       "      <td>-1046.729980</td>\n",
       "      <td>1716.910034</td>\n",
       "      <td>55.797798</td>\n",
       "      <td>24.788601</td>\n",
       "      <td>6.890150</td>\n",
       "      <td>5.813390</td>\n",
       "      <td>747.961975</td>\n",
       "      <td>994.250000</td>\n",
       "      <td>-412.966003</td>\n",
       "      <td>359.113007</td>\n",
       "      <td>175.209000</td>\n",
       "      <td>103.500999</td>\n",
       "      <td>84.447098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1488.199951</td>\n",
       "      <td>-25.370100</td>\n",
       "      <td>-30.989700</td>\n",
       "      <td>84.891502</td>\n",
       "      <td>26.878799</td>\n",
       "      <td>15.517200</td>\n",
       "      <td>13.260400</td>\n",
       "      <td>1415.640015</td>\n",
       "      <td>20.905100</td>\n",
       "      <td>223.630997</td>\n",
       "      <td>77.506500</td>\n",
       "      <td>57.986000</td>\n",
       "      <td>34.147400</td>\n",
       "      <td>26.660601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099995</th>\n",
       "      <td>1069.660034</td>\n",
       "      <td>659.874023</td>\n",
       "      <td>218.751007</td>\n",
       "      <td>126.183998</td>\n",
       "      <td>122.486000</td>\n",
       "      <td>27.608700</td>\n",
       "      <td>17.924801</td>\n",
       "      <td>-956.169006</td>\n",
       "      <td>-297.311005</td>\n",
       "      <td>-2204.350098</td>\n",
       "      <td>108.889999</td>\n",
       "      <td>21.177299</td>\n",
       "      <td>10.582400</td>\n",
       "      <td>9.138600</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099996</th>\n",
       "      <td>-1286.619995</td>\n",
       "      <td>-86.162498</td>\n",
       "      <td>-1366.270020</td>\n",
       "      <td>115.719002</td>\n",
       "      <td>109.853996</td>\n",
       "      <td>29.830200</td>\n",
       "      <td>22.489201</td>\n",
       "      <td>1145.729980</td>\n",
       "      <td>136.792007</td>\n",
       "      <td>1216.780029</td>\n",
       "      <td>489.053009</td>\n",
       "      <td>416.747009</td>\n",
       "      <td>84.599998</td>\n",
       "      <td>66.767502</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099997</th>\n",
       "      <td>-149.330002</td>\n",
       "      <td>1781.459961</td>\n",
       "      <td>-58.690899</td>\n",
       "      <td>508.045013</td>\n",
       "      <td>495.290985</td>\n",
       "      <td>82.283600</td>\n",
       "      <td>43.567902</td>\n",
       "      <td>84.726700</td>\n",
       "      <td>-1378.569946</td>\n",
       "      <td>-1485.469971</td>\n",
       "      <td>91.104897</td>\n",
       "      <td>79.120102</td>\n",
       "      <td>46.537300</td>\n",
       "      <td>23.227301</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099998</th>\n",
       "      <td>1584.699951</td>\n",
       "      <td>-731.156982</td>\n",
       "      <td>-196.348007</td>\n",
       "      <td>114.938004</td>\n",
       "      <td>83.769897</td>\n",
       "      <td>12.898200</td>\n",
       "      <td>9.031230</td>\n",
       "      <td>-1515.079956</td>\n",
       "      <td>783.245972</td>\n",
       "      <td>498.704010</td>\n",
       "      <td>553.737000</td>\n",
       "      <td>366.188995</td>\n",
       "      <td>192.139008</td>\n",
       "      <td>81.398201</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099999</th>\n",
       "      <td>-1229.599976</td>\n",
       "      <td>-1009.890015</td>\n",
       "      <td>1105.800049</td>\n",
       "      <td>154.110992</td>\n",
       "      <td>95.494904</td>\n",
       "      <td>76.178902</td>\n",
       "      <td>57.348499</td>\n",
       "      <td>1253.589966</td>\n",
       "      <td>967.965027</td>\n",
       "      <td>-233.518005</td>\n",
       "      <td>421.441986</td>\n",
       "      <td>238.966003</td>\n",
       "      <td>130.369995</td>\n",
       "      <td>88.144096</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                pxj1         pyj1         pzj1         mj1      tau1j1  \\\n",
       "0       -1467.239990   611.502014   511.101990   38.896000    8.290660   \n",
       "1       -1211.239990   347.315002   547.963013  389.532013  191.804001   \n",
       "2       -1229.619995   649.857971     8.089170   72.155502   47.168098   \n",
       "3        -693.304016 -1046.729980  1716.910034   55.797798   24.788601   \n",
       "4       -1488.199951   -25.370100   -30.989700   84.891502   26.878799   \n",
       "...              ...          ...          ...         ...         ...   \n",
       "1099995  1069.660034   659.874023   218.751007  126.183998  122.486000   \n",
       "1099996 -1286.619995   -86.162498 -1366.270020  115.719002  109.853996   \n",
       "1099997  -149.330002  1781.459961   -58.690899  508.045013  495.290985   \n",
       "1099998  1584.699951  -731.156982  -196.348007  114.938004   83.769897   \n",
       "1099999 -1229.599976 -1009.890015  1105.800049  154.110992   95.494904   \n",
       "\n",
       "            tau2j1     tau3j1         pxj2         pyj2         pzj2  \\\n",
       "0         4.836080   4.260190  1403.579956  -674.551025  -451.670990   \n",
       "1        99.562798  70.872200   619.341003   -62.177299 -1944.040039   \n",
       "2        37.243198  33.658199  1196.250000  -647.896973 -1283.109985   \n",
       "3         6.890150   5.813390   747.961975   994.250000  -412.966003   \n",
       "4        15.517200  13.260400  1415.640015    20.905100   223.630997   \n",
       "...            ...        ...          ...          ...          ...   \n",
       "1099995  27.608700  17.924801  -956.169006  -297.311005 -2204.350098   \n",
       "1099996  29.830200  22.489201  1145.729980   136.792007  1216.780029   \n",
       "1099997  82.283600  43.567902    84.726700 -1378.569946 -1485.469971   \n",
       "1099998  12.898200   9.031230 -1515.079956   783.245972   498.704010   \n",
       "1099999  76.178902  57.348499  1253.589966   967.965027  -233.518005   \n",
       "\n",
       "                mj2      tau1j2      tau2j2     tau3j2  label  \n",
       "0        237.893997   79.815102   21.010300  16.757601    0.0  \n",
       "1         22.999201    8.042190    6.335090   5.525360    0.0  \n",
       "2         78.230698   15.292900   13.944200  10.013500    0.0  \n",
       "3        359.113007  175.209000  103.500999  84.447098    0.0  \n",
       "4         77.506500   57.986000   34.147400  26.660601    0.0  \n",
       "...             ...         ...         ...        ...    ...  \n",
       "1099995  108.889999   21.177299   10.582400   9.138600    1.0  \n",
       "1099996  489.053009  416.747009   84.599998  66.767502    1.0  \n",
       "1099997   91.104897   79.120102   46.537300  23.227301    1.0  \n",
       "1099998  553.737000  366.188995  192.139008  81.398201    1.0  \n",
       "1099999  421.441986  238.966003  130.369995  88.144096    1.0  \n",
       "\n",
       "[1100000 rows x 15 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e97015a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau21jOne\"] = dataset[\"tau2jOne\"]/dataset[\"tau1jOne\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau21jOne\"] = dataset[\"tau2jOne\"]/dataset[\"tau1jOne\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau3jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau3jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n"
     ]
    }
   ],
   "source": [
    "dataset_bg=load_dataT21(features_bg)\n",
    "dataset_bg2=load_dataT21(features_bg2)\n",
    "dataset_sig=load_dataT21(features_sig)\n",
    "\n",
    "dataset_bgT32=load_dataT32(features_bg)\n",
    "dataset_bg2T32=load_dataT32(features_bg2)\n",
    "dataset_sigT32=load_dataT32(features_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7d2d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\259476259.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n"
     ]
    }
   ],
   "source": [
    "dataset_bg_more=load_data_more(features_bg)\n",
    "dataset_bg2_more=load_data_more(features_bg2)\n",
    "dataset_sig_more=load_data_more(features_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d4d30fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True ...  True  True  True]\n",
      "     # signal events:  1000\n",
      " signal significance:  4.059677443835412\n",
      "           s/b ratio:  0.016480980947986026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SR_low = 3.3\n",
    "SR_high = 3.7\n",
    "test_size = 0.5\n",
    "EPOCHS=20\n",
    "batch_size = 200\n",
    "SB_width = 0.3\n",
    "\n",
    "def sr_crit(d):\n",
    "    return (d[:,0] < SR_high) & (d[:,0] >= SR_low)\n",
    "\n",
    "\n",
    "\n",
    "bg_srsb, bg2_srsb, sig_srsb = sr_crit(dataset_bg_more), sr_crit(dataset_bg2_more), sr_crit(dataset_sig_more)\n",
    "\n",
    "SR_background_data = dataset_bg[bg_srsb]\n",
    "SB_background_data = dataset_bg[~bg_srsb]\n",
    "print(~bg_srsb)\n",
    "\n",
    "SR_background_sim = dataset_bg2[bg2_srsb]\n",
    "SB_background_sim = dataset_bg2[~bg2_srsb]\n",
    "\n",
    "SR_signal = dataset_sig[sig_srsb]\n",
    "SB_signal = dataset_sig[~sig_srsb]\n",
    "\n",
    "N_inputs = len(SR_background_data.T) - 1\n",
    "\n",
    "\n",
    "((SR_background_data, SR_background_data_fortest),\n",
    " (SR_background_sim, SR_background_sim_fortest),\n",
    " (SB_background_data, SB_background_data_fortest),\n",
    " (SB_background_sim, SB_background_sim_fortest),\n",
    " ) = [train_test_split(arr, test_size=test_size) for arr in [\n",
    "    SR_background_data, SR_background_sim,\n",
    "    SB_background_data, SB_background_sim,\n",
    "]]\n",
    "\n",
    "mn,mx = np.percentile(np.concatenate([SB_background_data, SB_background_sim, SR_background_data, \n",
    "                                      SR_background_sim]), [1,99], axis=0)\n",
    "\n",
    "def norm_func(d):\n",
    "    return (d - mn)/(mx - mn)\n",
    "\n",
    "Nsig = 1000\n",
    "Nsig_SB = int(np.round(len(SB_signal)*Nsig/len(SR_signal)))\n",
    "\n",
    "fmt = '{:>20}:  {}'\n",
    "print(fmt.format('# signal events', Nsig))\n",
    "print(fmt.format('signal significance', len(SR_signal[0:Nsig])/len(SR_background_data)**0.5))\n",
    "print(fmt.format('s/b ratio', len(SR_signal[0:Nsig])/len(SR_background_data)))\n",
    "\n",
    "n_injections = 20\n",
    "SR_signals_to_inject = [None] * n_injections\n",
    "SB_signals_to_inject = [None] * n_injections\n",
    "signals_to_test = [None] * n_injections\n",
    "\n",
    "for i in range(n_injections):\n",
    "    idx = np.isin(range(len(SR_signal)), np.random.choice(SR_signal.shape[0], size=Nsig, replace=False))\n",
    "    SR_signals_to_inject[i] = SR_signal[idx, :]\n",
    "    signals_to_test[i] = SR_signal[~idx, :]\n",
    "    \n",
    "    idx = np.isin(range(len(SB_signal)), np.random.choice(SB_signal.shape[0], size=Nsig_SB, replace=False))\n",
    "    SB_signals_to_inject[i] = SB_signal[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed71325e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SB_signals_to_inject' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSB_signals_to_inject\u001b[49m[i]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SB_signals_to_inject' is not defined"
     ]
    }
   ],
   "source": [
    "SB_signals_to_inject[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3172627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                               | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      "  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                 | 1/20 [01:22<26:14, 82.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                           | 2/20 [02:45<24:47, 82.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                     | 3/20 [04:09<23:33, 83.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                               | 4/20 [05:32<22:11, 83.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                         | 5/20 [06:58<21:01, 84.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                   | 6/20 [08:21<19:31, 83.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                             | 7/20 [09:43<18:02, 83.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                       | 8/20 [11:09<16:51, 84.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 9/20 [12:35<15:30, 84.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                           | 10/20 [13:58<14:02, 84.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                     | 11/20 [15:22<12:36, 84.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                               | 12/20 [16:47<11:14, 84.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 13/20 [18:15<09:58, 85.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 14/20 [19:40<08:32, 85.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 15/20 [21:04<07:04, 84.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 16/20 [22:28<05:38, 84.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 17/20 [23:52<04:13, 84.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 18/20 [25:21<02:51, 85.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "\r",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 19/20 [26:48<01:26, 86.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2394776141.py:48: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [28:14<00:00, 84.71s/it]\n"
     ]
    }
   ],
   "source": [
    "#Next is CWoLa\n",
    "tpr_pts = np.linspace(0, 1, 10000)\n",
    "cwola_rocs = pd.DataFrame(columns=['fpr', 'tpr'])\n",
    "\n",
    "for i in tqdm.tqdm(range(n_injections)):\n",
    "    \n",
    "    SR_signal_to_inject = SR_signals_to_inject[i]\n",
    "    SB_signal_to_inject = SB_signals_to_inject[i]\n",
    "\n",
    "    signal_to_test = signals_to_test[i]\n",
    "        \n",
    "    X_cwola_train = np.concatenate([SR_signal_to_inject, SR_background_data, SB_signal_to_inject, SB_background_data])\n",
    "    Y_cwola_train = np.concatenate([np.ones(len(SR_signal_to_inject)), np.ones(len(SR_background_data)),\n",
    "                                    np.zeros(len(SB_signal_to_inject)), np.zeros(len(SB_background_data))])\n",
    "    \n",
    "    X_cwola_train, Y_cwola_train = shuffle(X_cwola_train, Y_cwola_train, )\n",
    "\n",
    "    sbmjj = X_cwola_train[:,0]\n",
    "    sb_crit = ((sbmjj > SR_low - SB_width)&(sbmjj <= SR_high + SB_width))\n",
    "    \n",
    "    X_cwola_train = X_cwola_train[sb_crit]\n",
    "    Y_cwola_train = Y_cwola_train[sb_crit]\n",
    "    \n",
    "    w_low = 0.5*(len(SR_signal_to_inject) + len(SR_background_data))/((X_cwola_train[:,0] <= SR_low).sum())\n",
    "    w_high = 0.5*(len(SR_signal_to_inject) + len(SR_background_data))/((X_cwola_train[:,0] >= SR_high).sum())\n",
    "\n",
    "    W_cwola_train = np.ones_like(Y_cwola_train)\n",
    "    W_cwola_train[(Y_cwola_train == 0) & (X_cwola_train[:,0] <= SR_low)] = w_low\n",
    "    W_cwola_train[(Y_cwola_train == 0) & (X_cwola_train[:,0] >= SR_high)] = w_high\n",
    "    \n",
    "    X_cwola_val = np.concatenate([signal_to_test,SR_background_data_fortest])\n",
    "    Y_cwola_val = np.concatenate([np.ones(len(signal_to_test)),np.zeros(len(SR_background_data_fortest))])\n",
    "    \n",
    "    K.clear_session()\n",
    "    model_cwola = Sequential()\n",
    "    model_cwola.add(Dense(64, input_dim=N_inputs, activation='relu')) \n",
    "    model_cwola.add(Dense(64, activation='relu'))\n",
    "    model_cwola.add(Dense(64, activation='relu'))\n",
    "    model_cwola.add(Dense(1, activation='sigmoid'))\n",
    "    model_cwola.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    hist_cwola = model_cwola.fit(X_cwola_train[:,1:], Y_cwola_train, epochs=EPOCHS, \n",
    "                                 batch_size=batch_size, verbose=0, sample_weight=W_cwola_train)\n",
    "\n",
    "    scores_cwola = model_cwola.predict(X_cwola_val[:,1:],batch_size=batch_size)\n",
    "        \n",
    "    fpr_cwola, tpr_cwola, _ = roc_curve(Y_cwola_val, scores_cwola)\n",
    "    cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e13f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_cwola.save('/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/hybrid_method/output/cwola_clasiffier.h5')\n",
    "\n",
    "#scores_cwola = model_cwola.predict(X_cwola_val[:,1:],batch_size=batch_size)\n",
    "        \n",
    "#fpr_cwola, tpr_cwola, _ = roc_curve(Y_cwola_val, scores_cwola)\n",
    "#cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c705260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2792228521.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.keras.models.load_model('/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/hybrid_method/output/cwola_clasiffier.h5')\n",
    "\n",
    "fpr_cwola, tpr_cwola, _ = roc_curve(Y_cwola_val, scores_cwola)\n",
    "cwola_rocs = cwola_rocs.append({'fpr': fpr_cwola, 'tpr': tpr_cwola}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d070916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\3456826973.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  fpr_interp = np.ma.masked_invalid(1./np.array(fpr_interp))\n",
      "C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:755: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n"
     ]
    }
   ],
   "source": [
    "# CWoLa\n",
    "fpr_interp = [None] * len(cwola_rocs)\n",
    "\n",
    "for i, row in cwola_rocs.iterrows():\n",
    "    fpr, tpr = row\n",
    "    interp = interpolate.interp1d(tpr, fpr, fill_value=float('nan'), bounds_error=False, assume_sorted=True)\n",
    "\n",
    "    fpr_pts = interp(tpr_pts)\n",
    "    fpr_interp[i] = fpr_pts\n",
    "\n",
    "fpr_interp = np.ma.masked_invalid(1./np.array(fpr_interp))\n",
    "\n",
    "cwola_max = np.nanmax(fpr_interp, axis=0).data\n",
    "cwola_min = np.nanmin(fpr_interp, axis=0).data\n",
    "cwola_med = np.median(fpr_interp, axis=0).data\n",
    "cwola_mean = np.nanmean(fpr_interp, axis=0).data\n",
    "cwola_std = np.nanstd(fpr_interp, axis=0).data\n",
    "    \n",
    "sig_interp = np.ma.masked_invalid(np.sqrt(fpr_interp))\n",
    "    \n",
    "cwola_smean = np.nanmean(sig_interp, axis=0).data\n",
    "cwola_sstd = np.nanstd(sig_interp, axis=0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4b933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "def load_dataT(input_frame):\n",
    "    dataset = input_frame[[\"mj1\",\"tau1j1\",\"tau2j1\",\"mj2\",\"tau1j2\",\"tau2j2\",\"tau3j1\",\"tau3j2\",\"label\"]]\n",
    "    #Usar esta, masa al cuadrado\n",
    "    dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
    "    \n",
    "    \n",
    "    dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
    "    dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
    "    \n",
    "    \n",
    "    #T_{21} Jet 1\n",
    "    dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
    "    dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau21jOne\"] = dataset[\"tau2jOne\"]/dataset[\"tau1jOne\"]\n",
    "    \n",
    "    #T_{21} Jet 2\n",
    "    dataset[\"tau1jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
    "    dataset[\"tau2jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n",
    "    dataset[\"tau21jTwo\"] = dataset[\"tau2jTwo\"]/dataset[\"tau1jTwo\"]\n",
    "    \n",
    "    #T_{32} Jet 1\n",
    "    dataset[\"tau3jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
    "    dataset[\"tau32jOne\"] = dataset[\"tau3jOne\"]/dataset[\"tau2jOne\"]\n",
    "    \n",
    "    \n",
    "    #T_{32} Jet 2\n",
    "    dataset[\"tau3jTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"tau3j1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"tau3j2\"]\n",
    "    dataset[\"tau32jTwo\"] = dataset[\"tau3jTwo\"]/dataset[\"tau2jTwo\"]    \n",
    "    \n",
    "    #pT\n",
    "    dataset[\"pT1\"] = (input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2)**0.5/1000.\n",
    "    dataset[\"pT2\"] = (input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2)**0.5/1000.\n",
    "    \n",
    "    #cada jet\n",
    "    dataset[\"pTjOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"pT1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"pT2\"]\n",
    "    dataset[\"pTjOne\"] = dataset[\"pTjOne\"]*1000\n",
    "    \n",
    "    dataset[\"pTjTwo\"] = (dataset[\"mjTwo\"] == dataset[\"mj1\"])*dataset[\"pT1\"]+(dataset[\"mjTwo\"] == dataset[\"mj2\"])*dataset[\"pT2\"] \n",
    "    dataset[\"pTjTwo\"] = dataset[\"pTjTwo\"]*1000\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(dataset[\"pTjOne\"])\n",
    "    #print((dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"pT1\"])\n",
    "    \n",
    "    dataset[\"rhopjOne\"] = np.log(dataset[\"mjj\"]/dataset[\"pTjOne\"])\n",
    "    dataset[\"rhopjTwo\"] = np.log(dataset[\"mjj\"]/dataset[\"pTjTwo\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset[\"mjTwo\"] = dataset[\"mjTwo\"]/1000. + 2*alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjOne\"] = dataset[\"mjOne\"]/1000. + alpha*dataset[\"mjj\"]\n",
    "    dataset[\"mjDelta\"] = (dataset[\"mjTwo\"] - dataset[\"mjOne\"])\n",
    "\n",
    "    \n",
    "\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = dataset[[\"tau21jOne\",\"tau21jTwo\",\"tau32jOne\",\"tau32jTwo\",\"label\"]]\n",
    "    #dataset = dataset[[\"mjj\",\"mjOne\",\"mjDelta\", \"tau21jOne\", \"tau21jTwo\", \"tau32jOne\", \"tau32jTwo\",\"pTjOne\",\"pTjTwo\",\"Dy\"]]\n",
    "    \n",
    "    \n",
    "    dataset = dataset.sort_index()\n",
    "    #print(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323de1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2374451860.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjj\"] = (((input_frame[\"pxj1\"]**2+input_frame[\"pyj1\"]**2+input_frame[\"pzj1\"]**2+input_frame[\"mj1\"]**2)**0.5+(input_frame[\"pxj2\"]**2+input_frame[\"pyj2\"]**2+input_frame[\"pzj2\"]**2+input_frame[\"mj2\"]**2)**0.5)**2-(input_frame[\"pxj1\"]+input_frame[\"pxj2\"])**2-(input_frame[\"pyj1\"]+input_frame[\"pyj2\"])**2-(input_frame[\"pzj1\"]+input_frame[\"pzj2\"])**2)**0.5/1000.\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2374451860.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjTwo\"] = dataset[[\"mj1\", \"mj2\"]].max(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2374451860.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"mjOne\"] = dataset[[\"mj1\", \"mj2\"]].min(axis=1)\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2374451860.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau1jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau1j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau1j2\"]\n",
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\2374451860.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"tau2jOne\"] = (dataset[\"mjOne\"] == dataset[\"mj1\"])*dataset[\"tau2j1\"]+(dataset[\"mjOne\"] == dataset[\"mj2\"])*dataset[\"tau2j2\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Could not find history file {}.\n",
      "1075/1075 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\1773702939.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ann_rocs = ann_rocs.append({'fpr': fpr_ann, 'tpr': tpr_ann}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # Define variable(s)\n",
    "name    = 'classifier'\n",
    "basedir = '/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/hybrid_method/output/'\n",
    "\n",
    "ann_rocs = pd.DataFrame(columns=['fpr', 'tpr'])\n",
    "\n",
    "\n",
    "\n",
    "data = load_dataT(original_data)\n",
    "\n",
    "X = data[[\"tau21jOne\",\"tau21jTwo\",\"tau32jOne\",\"tau32jTwo\"]].values\n",
    "Y = data[\"label\"].values\n",
    "\n",
    "\n",
    "classifier, history = load(basedir, name)\n",
    "\n",
    "# Predict\n",
    "scores_ann = classifier.predict(X, batch_size = 1024) #X reemplazar por datos CWola\n",
    "\n",
    "\n",
    "fpr_ann, tpr_ann, _ = roc_curve(Y, scores_ann)\n",
    "\n",
    "\n",
    "ann_rocs = ann_rocs.append({'fpr': fpr_ann, 'tpr': tpr_ann}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2cc4fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\AppData\\Local\\Temp\\ipykernel_20460\\1081347376.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  fpr_interp = np.ma.masked_invalid(1./np.array(fpr_interp))\n",
      "C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:755: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n"
     ]
    }
   ],
   "source": [
    "# ANN\n",
    "fpr_interp = [None] * len(ann_rocs)\n",
    "    \n",
    "for i, row in ann_rocs.iterrows():\n",
    "    fpr, tpr = row\n",
    "    interp = interpolate.interp1d(tpr, fpr, fill_value=float('nan'), bounds_error=False, assume_sorted=True)\n",
    "\n",
    "    fpr_pts = interp(tpr_pts)\n",
    "    fpr_interp[i] = fpr_pts\n",
    "\n",
    "fpr_interp = np.ma.masked_invalid(1./np.array(fpr_interp))\n",
    "\n",
    "ann_max = np.nanmax(fpr_interp, axis=0).data\n",
    "ann_min = np.nanmin(fpr_interp, axis=0).data\n",
    "ann_med = np.median(fpr_interp, axis=0).data\n",
    "ann_mean = np.nanmean(fpr_interp, axis=0).data\n",
    "ann_std = np.nanstd(fpr_interp, axis=0).data\n",
    "   \n",
    "sig_interp = np.ma.masked_invalid(np.sqrt(fpr_interp))\n",
    "    \n",
    "ann_smean = np.nanmean(sig_interp, axis=0).data\n",
    "ann_sstd = np.nanstd(sig_interp, axis=0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729afc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Cadena de caracteres en el formato JSON\n",
    "datos_JSON =  \"\"\"\n",
    "{\n",
    "    \"config\": \"./configs/configs.json\",\n",
    "    \"devices\": 1,\n",
    "    \"gpu\": true,\n",
    "    \"input\": \"C:/Users/knukl/OneDrive/TEC/ESTANCIA/Final/FINAL/input/\",\n",
    "    \"jobname\": \"\",\n",
    "    \"mode\": \"gpu\",\n",
    "    \"optimise_adversarial\": false,\n",
    "    \"optimise_classifier\": true,\n",
    "    \"output\": \"./output/\",\n",
    "    \"patches\": [],\n",
    "    \"tensorboard\": false,\n",
    "    \"theano\": false,\n",
    "    \"train\": false,\n",
    "    \"train_adversarial\": false,\n",
    "    \"train_classifier\": true,\n",
    "    \"verbose\": false\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Convertir cadena de caracteres JSON a un diccionario\n",
    "datos_diccionario = json.loads(datos_JSON)\n",
    "\n",
    "print(datos_diccionario['gpu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df32252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': {'units': 64, 'activation': 'relu', 'batchnorm': True}, 'architecture': 3}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "parse_datos = \"\"\"\n",
    "{\n",
    "    \"classifier\": {\n",
    "      \t\"model\": {\n",
    "      \t    \"default\": {\n",
    "            \t\"units\":      64,\n",
    "            \t\"activation\": \"relu\",\n",
    "            \t\"batchnorm\":  true\n",
    "      \t    },\n",
    "      \t    \"architecture\": 3\n",
    "      \t},\n",
    "      \t\"fit\": {\n",
    "      \t    \"epochs\":     200,\n",
    "      \t    \"batch_size\": 2048,\n",
    "      \t    \"shuffle\":    true\n",
    "      \t},\n",
    "      \t\"compile\" : {\n",
    "      \t    \"loss\":      \"binary_crossentropy\",\n",
    "      \t    \"lr\":        1.0E-02,\n",
    "      \t    \"decay\":     1.0E-03,\n",
    "      \t    \"optimizer\": \"Adam\",\n",
    "\t\t\t\"weighted_metrics\": []\n",
    "      \t}\n",
    "    },\n",
    "    \"adversary\": {\n",
    "      \t\"model\": {\n",
    "      \t    \"default\": {\n",
    "            \t\"units\":      64,\n",
    "            \t\"activation\": \"relu\"\n",
    "      \t    },\n",
    "      \t    \"architecture\": 1,\n",
    "      \t    \"gmm_components\": 20\n",
    "      \t},\n",
    "      \t\"fit\": {},\n",
    "      \t\"compile\": {}\n",
    "    },\n",
    "    \"combined\": {\n",
    "\t\"pretrain\": 10,\n",
    "      \t\"model\": {\n",
    "      \t    \"lambda_reg\": 10.0,\n",
    "      \t    \"lr_ratio\" : null\n",
    "      \t},\n",
    "      \t\"fit\": {\n",
    "      \t    \"epochs\": 200,\n",
    "      \t    \"batch_size\": 2048,\n",
    "      \t    \"shuffle\": true\n",
    "      \t},\n",
    "      \t\"compile\": {\n",
    "            \"loss\":         [\"binary_crossentropy\", null],\n",
    "      \t    \"loss_weights\": [2.0E-07, 1.0E+00],\n",
    "            \"lr\":           5.0E-02,\n",
    "            \"decay\":        1.0E-02,\n",
    "            \"optimizer\":    \"Adam\"\n",
    "      \t}\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cfg = json.loads(parse_datos)\n",
    "print(cfg['classifier']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8dc629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': {'units': 64, 'activation': 'relu'}, 'architecture': 1, 'gmm_components': 20}\n",
      "Model: \"adversary\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " adversary/input_clf (InputLaye  [(None, 1)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " adversary/input_pt (InputLayer  [(None, 2)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1)           4           ['adversary/input_clf[0][0]']    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 2)            0           ['adversary/input_pt[0][0]']     \n",
      "                                                                                                  \n",
      " adversary/concatenate (Concate  (None, 3)           0           ['batch_normalization[0][0]',    \n",
      " nate)                                                            'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " adversary/coeffs (Dense)       (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/means_1 (Dense)      (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/means_2 (Dense)      (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/means_3 (Dense)      (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/means_4 (Dense)      (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/widths_1 (Dense)     (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/widths_2 (Dense)     (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/widths_3 (Dense)     (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/widths_4 (Dense)     (None, 20)           80          ['adversary/concatenate[0][0]']  \n",
      "                                                                                                  \n",
      " adversary/input_mass (InputLay  [(None, 4)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " adversary/output (PosteriorLay  (None,)             0           ['adversary/coeffs[0][0]',       \n",
      " er)                                                              'adversary/means_1[0][0]',      \n",
      "                                                                  'adversary/means_2[0][0]',      \n",
      "                                                                  'adversary/means_3[0][0]',      \n",
      "                                                                  'adversary/means_4[0][0]',      \n",
      "                                                                  'adversary/widths_1[0][0]',     \n",
      "                                                                  'adversary/widths_2[0][0]',     \n",
      "                                                                  'adversary/widths_3[0][0]',     \n",
      "                                                                  'adversary/widths_4[0][0]',     \n",
      "                                                                  'adversary/input_mass[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 724\n",
      "Trainable params: 722\n",
      "Non-trainable params: 2\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Lambda, Concatenate\n",
    "# Basic imports\n",
    "import glob\n",
    "import pickle\n",
    "import logging as log\n",
    "import itertools\n",
    "\n",
    "# Scientific imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tkinter\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from scipy import stats, interpolate\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "# Project imports\n",
    "from adver.utilidad import *\n",
    "from adver.profile import *\n",
    "from adver.constants import *\n",
    "from adver.layers import *\n",
    "from RUN.common import *\n",
    "from RUN.comis import *\n",
    "\n",
    "def layer_name_factory (scope):\n",
    "    \"\"\" ... \"\"\"\n",
    "    def layer_name (name):\n",
    "        if scope:\n",
    "            return '{}/{}'.format(scope, name)\n",
    "        return name\n",
    "    return layer_name\n",
    "\n",
    "\n",
    "def keras_layer_name_factory (scope):\n",
    "    \"\"\" ... \"\"\"\n",
    "    layer_name = layer_name_factory(scope)\n",
    "    def keras_layer_name (cls):\n",
    "        return layer_name('{}_{}'.format(snake_case(cls), K.get_uid(cls)))\n",
    "    return keras_layer_name\n",
    "\n",
    "\n",
    "def stack_layers (input_layer, architecture, default, scope=None):\n",
    "\n",
    "\n",
    "    # Method(s) to get name of layers\n",
    "    keras_layer_name = keras_layer_name_factory(scope)\n",
    "\n",
    "    # Prepare first layer\n",
    "    l = input_layer\n",
    "\n",
    "    # Loop layer specifications\n",
    "    for spec in architecture:\n",
    "        print(architecture)\n",
    "        print(spec)\n",
    "\n",
    "        # Update the specifications of the current layer to include any defaults\n",
    "        opts = dict(**default)\n",
    "        #print(default)\n",
    "        #print(opts)\n",
    "        opts.update(spec)\n",
    "        #print(opts)\n",
    "        #opts = {\"units\": 64, \"activation\": \"relu\", \"batchnorm\": True}\n",
    "\n",
    "        # Extract non-standard keyword arguments\n",
    "        batchnorm = opts.pop('batchnorm', False)\n",
    "        dropout   = opts.pop('dropout',   None)\n",
    "        l1reg     = opts.pop('l1reg',     None)\n",
    "        l2reg     = opts.pop('l2reg',     None)\n",
    "\n",
    "        # 1: (Opt.) Add batch normalisation layer before dense layer\n",
    "        if batchnorm:\n",
    "            l = BatchNormalization(name=keras_layer_name('BatchNormalization'))(l)\n",
    "            pass\n",
    "\n",
    "        # 2: Add dense layer according to specifications\n",
    "        l = Dense(name=keras_layer_name('Dense'),\n",
    "                  activity_regularizer=regularizers.l1(l1reg) if l1reg else None,\n",
    "                  kernel_regularizer  =regularizers.l2(l2reg) if l2reg else None,\n",
    "                  **opts)(l)\n",
    "\n",
    "        # 3: (Opt.) Add dropout regularisation layer after dense layer\n",
    "        if dropout:\n",
    "            l = Dropout(dropout, seed=RNG.randint(np.iinfo(np.int).max), name=keras_layer_name('Dropout'))(l)\n",
    "            pass\n",
    "\n",
    "        pass\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "def adversary_model (gmm_dimensions, gmm_components=None, architecture=[], default=dict(), scope='adversary'):\n",
    "\n",
    "\n",
    "    # Method(s) to get name of layers\n",
    "    keras_layer_name = keras_layer_name_factory(scope)\n",
    "    layer_name       = layer_name_factory(scope)\n",
    "\n",
    "    # Input(s)\n",
    "    adversary_input_clf = Input(shape=(1,),              name=layer_name('input_clf'))\n",
    "    adversary_input_pt  = Input(shape=(2,),              name=layer_name('input_pt'))  # @TEMP # Seran 2 para cada logjet?\n",
    "    adversary_input_mass = Input(shape=(gmm_dimensions,), name=layer_name('input_mass')) # Mass input\n",
    "\n",
    "    # Batch-normalise classifier output\n",
    "    #clf = adversary_input_clf  # BatchNormalization()(adversary_input_clf)\n",
    "    clf = BatchNormalization()(adversary_input_clf)\n",
    "    # Re-scale input pt\n",
    "    pt = BatchNormalization()(adversary_input_pt)\n",
    "    pt = Lambda(lambda pt: (pt - np.log(200.))/(np.log(2000.) - np.log(200.)))(adversary_input_pt)\n",
    "\n",
    "    # Intermediate layer(s)\n",
    "    inputs = Concatenate(name=layer_name('concatenate'))([clf, pt])\n",
    "    features = stack_layers(inputs, architecture, default, scope=scope)\n",
    "\n",
    "    # Posterior p.d.f. parameters\n",
    "    print(gmm_components)\n",
    "    r_coeffs = Dense(20, name=layer_name('coeffs'), activation='softmax')(features)\n",
    "    r_means  = list()\n",
    "    r_widths = list()\n",
    "    for i in range(1, gmm_dimensions + 1): # I TOOK THE X FROM XRANGE\n",
    "        # Activation: Require all means to be in [0,1]\n",
    "        r_means .append( Dense(20, activation='sigmoid',  name=layer_name('means_{}'.format(i)))(features) )\n",
    "        pass\n",
    "    for i in range(1, gmm_dimensions + 1):\n",
    "        # Require all widths to be positive\n",
    "        r_widths.append( Dense(20, activation='softplus', name=layer_name('widths_{}'.format(i)))(features) )\n",
    "        pass\n",
    "\n",
    "    # Posterior probability layer\n",
    "    adversary_output = PosteriorLayer(20, gmm_dimensions, name=layer_name('output'))([r_coeffs] + r_means + r_widths + [adversary_input_mass])\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=[adversary_input_clf, adversary_input_pt, adversary_input_mass],\n",
    "                  outputs=adversary_output,\n",
    "                  name=scope)\n",
    "    # Return\n",
    "    return model\n",
    "\n",
    "def combined_model (classifier, adversary, lambda_reg=None, lr_ratio=None, scope='combined'):\n",
    "\n",
    "    keras_layer_name = keras_layer_name_factory(scope)\n",
    "    layer_name       = layer_name_factory(scope)\n",
    "\n",
    "    # Reconstruct classifier\n",
    "    classifier_input = classifier.layers[0]\n",
    "    print(classifier_input)\n",
    "\n",
    "    combined_input_clf  = Input(shape=classifier_input.input_shape[0][1], name=layer_name(classifier_input.name.replace('/', '_')))\n",
    "    combined_output_clf = classifier(combined_input_clf)\n",
    "    \n",
    "    # Add gradient reversal layer\n",
    "    gradient_reversal = GradientReversalLayer(lambda_reg * lr_ratio, name=keras_layer_name('GradientReversalLayer'))(combined_output_clf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Reconstruct adversary\n",
    "    input_layers   = filter(lambda l: type(l) == InputLayer, adversary.layers)\n",
    "    adversary_input_pt  = list(filter(lambda l: l.name.endswith('_pt'),  input_layers))[0]\n",
    "    adversary_input_mass = list(adversary.layers)[9]    \n",
    "    \n",
    "    inputs_adv = [\n",
    "        Input(shape=(2,), name=layer_name(adversary_input_pt .name.replace('/', '_'))),\n",
    "        Input(shape=(1,), name=layer_name(adversary_input_mass.name.replace('/', '_'))),\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    outputs_adv = [adversary([gradient_reversal] + inputs_adv)]\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs =[combined_input_clf]  + inputs_adv,\n",
    "                  outputs=[combined_output_clf] + outputs_adv,\n",
    "                  name=scope)\n",
    "\n",
    "    # Return\n",
    "    return model\n",
    "\n",
    "#adver_model = tf.keras.models.load_model('/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/hybrid_method/output/adversary_lambda10.h5')\n",
    "classifier = tf.keras.models.load_model('/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/hybrid_method/output/cwola_clasiffier.h5')\n",
    "adversary = adversary_model(4, cfg['adversary']['model'])\n",
    "adversary.summary()\n",
    "#hybrid = combined_model(classifier, adversary, **cfg['combined']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_classifier = classifier.layers[:-1]  \n",
    "layers_adversary = adversary.layers[1:]  # Excluye la primera capa del modelo 2\n",
    "\n",
    "# Crear un nuevo modelo combinado\n",
    "hybrid = tf.keras.models.Sequential()\n",
    "for layer in layers_classifier:\n",
    "    hybrid.add(layer)\n",
    "    \n",
    "hybrid.summary()\n",
    "for layer in layers_adversary:\n",
    "    print(\"a\")\n",
    "    hybrid.add(layer)\n",
    "\n",
    "# Compilar el modelo combinado\n",
    "hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Realizar otras configuraciones o entrenamientos adicionales si es necesario\n",
    "#hybrid.fit()\n",
    "\n",
    "# Guardar el modelo combinado\n",
    "#modelo_combinado.save('ruta_del_archivo/modelo_combinado.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17512548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:327: UserWarning: adver.models is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"combined\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " combined/classifier_input (Inp  [(None, 4)]         0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " classifier (Functional)        (None, 1)            9233        ['combined/classifier_input[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " GradientReversalLayer (Gradien  (None, 1)           0           ['classifier[0][0]']             \n",
      " tReversalLayer)                                                                                  \n",
      "                                                                                                  \n",
      " combined/adversary_input_pt (I  [(None, 2)]         0           []                               \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " combined/adversary_input_mass   [(None, 1)]         0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " adversary (Functional)         (None,)              4160        ['GradientReversalLayer[0][0]',  \n",
      "                                                                  'combined/adversary_input_pt[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'combined/adversary_input_mass[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,393\n",
      "Trainable params: 13,127\n",
      "Non-trainable params: 266\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import losses\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__ (self, hp_lambda=np.nan, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def call (self, x, mask=None):\n",
    "        return self.gr_op(x)\n",
    "\n",
    "    @tf.function\n",
    "    def gr_op(self, x):\n",
    "        return tf.negative(x)  # Invierte los gradientes multiplicÃ¡ndolos por -1\n",
    "\n",
    "    def get_config (self):\n",
    "        config = {\"name\": self.__class__.__name__, \"hp_lambda\": self.hp_lambda}\n",
    "        base_config = super(GradientReversalLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class PosteriorLayer (Layer):\n",
    "    \"\"\"Custom layer, modelling the posterior probability distribution for the jet mass using a gaussian mixture model (GMM)\"\"\"\n",
    "\n",
    "    # @TODO:\n",
    "    # - Check that K.sum((x < 0) || (x > 1)) == 0\n",
    "\n",
    "    def __init__ (self, gmm_components=np.nan, gmm_dimensions=np.nan, **kwargs):\n",
    "        self.output_dim = 1 # Probability\n",
    "        self.gmm_components = gmm_components\n",
    "        self.gmm_dimensions = gmm_dimensions\n",
    "        super(PosteriorLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call (self, x, mask=None):\n",
    "        \"\"\"Main call-method of the layer.\n",
    "\n",
    "        The GMM needs to be implemented (1) within this method and (2) using\n",
    "        Keras backend functions in order for the error back-propagation to work\n",
    "        properly.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check(s)\n",
    "        #mask = (x < 0) | (x > 1)\n",
    "        #assert K.sum(mask) == 0, \"Received input to PosteriorLayer outside of [0,1]: \" + str(K.eval(x[mask]))\n",
    "\n",
    "        # Unpack list of inputs\n",
    "        coeffs = x[0]\n",
    "        means  = x[1:                            1 + 1 * self.gmm_dimensions]\n",
    "        widths = x[1 + 1 * self.gmm_dimensions : 1 + 2 * self.gmm_dimensions]\n",
    "        inputs = x[-1]\n",
    "\n",
    "        # Compute the pdf from the GMM\n",
    "        pdf  = gaussian(inputs[:,0], coeffs[:,0], means[0][:,0], widths[0][:,0])\n",
    "        pdf /= gaussian_integral_on_unit_interval(means[0][:,0], widths[0][:,0])\n",
    "        for d in range(1, self.gmm_dimensions):\n",
    "            pdf *= gaussian(inputs[:,d], 1,           means[d][:,0], widths[d][:,0])\n",
    "            pdf /= gaussian_integral_on_unit_interval(means[d][:,0], widths[d][:,0])\n",
    "            pass\n",
    "        for c in range(1, self.gmm_components):\n",
    "            this_pdf  = gaussian(inputs[:,0], coeffs[:,c], means[0][:,c], widths[0][:,c])\n",
    "            this_pdf /= gaussian_integral_on_unit_interval(means[0][:,c], widths[0][:,c])\n",
    "            for d in range(1, self.gmm_dimensions):\n",
    "                this_pdf *= gaussian(inputs[:,d], 1,           means[d][:,c], widths[d][:,c])\n",
    "                this_pdf /= gaussian_integral_on_unit_interval(means[d][:,c], widths[d][:,c])\n",
    "                pass\n",
    "            pdf += this_pdf\n",
    "            pass\n",
    "\n",
    "        return K.reshape(pdf, (K.shape(pdf)[0],))\n",
    "\n",
    "    def compute_output_shape (self, input_shape):\n",
    "        return (input_shape[0][0], self.output_dim)\n",
    "\n",
    "    def get_config (self):\n",
    "        config = {\"name\": self.__class__.__name__,\n",
    "                  \"gmm_components\": self.gmm_components,\n",
    "                  \"gmm_dimensions\": self.gmm_dimensions}\n",
    "        base_config = super(PosteriorLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def kl_divergence(y_true, y_pred):\n",
    "    kl_loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "    return kl_loss\n",
    "\n",
    "# Registra la funciÃ³n de pÃ©rdida personalizada\n",
    "custom_objects = {'kl_divergence': kl_divergence, 'GradientReversalLayer': GradientReversalLayer, 'PosteriorLayer': PosteriorLayer}\n",
    "\n",
    "# Carga el modelo con los objetos personalizados registrados\n",
    "hybrid_model = tf.keras.models.load_model('/Users/bolit/All/Documentos/Tec/7_semestre/investigacion/CWoLa_ANN_Analysis/hybrid_method/output/combined_lambda10.h5', custom_objects=custom_objects, compile=False)\n",
    "hybrid_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0eaac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162734, 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cwola_train[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9e233641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"combined\" is incompatible with the layer: expected shape=(None, 2), found shape=(None, 4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m hybrid_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], loss_weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2.0E-07\u001b[39m, \u001b[38;5;241m1.0E+00\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_cwola_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cwola_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cwola_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_cwola_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW_cwola_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filefgdve7go.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\bolit\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"combined\" is incompatible with the layer: expected shape=(None, 2), found shape=(None, 4)\n"
     ]
    }
   ],
   "source": [
    "hybrid_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], loss_weights=[2.0E-07, 1.0E+00])\n",
    "\n",
    "hybrid_model.fit([X_cwola_train[:,1:], X_cwola_train[:,1:], X_cwola_train[:,1:]], Y_cwola_train, epochs=EPOCHS, \n",
    "                                 batch_size=batch_size, verbose=2, sample_weight=W_cwola_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94fc27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN hybird\n",
    "fpr_interp = [None] * len(hybrid_rocs)\n",
    "    \n",
    "for i, row in hybrid_rocs.iterrows():\n",
    "    fpr, tpr = row\n",
    "    interp = interpolate.interp1d(tpr, fpr, fill_value=float('nan'), bounds_error=False, assume_sorted=True)\n",
    "\n",
    "    fpr_pts = interp(tpr_pts)\n",
    "    fpr_interp[i] = fpr_pts\n",
    "\n",
    "fpr_interp = np.ma.masked_invalid(1./np.array(fpr_interp))\n",
    "\n",
    "hybrid_max = np.nanmax(fpr_interp, axis=0).data\n",
    "hybrid_min = np.nanmin(fpr_interp, axis=0).data\n",
    "hybrid_med = np.median(fpr_interp, axis=0).data\n",
    "hybrid_mean = np.nanmean(fpr_interp, axis=0).data\n",
    "hybrid_std = np.nanstd(fpr_interp, axis=0).data\n",
    "   \n",
    "sig_interp = np.ma.masked_invalid(np.sqrt(fpr_interp))\n",
    "    \n",
    "hybrid_smean = np.nanmean(sig_interp, axis=0).data\n",
    "hybrid_sstd = np.nanstd(sig_interp, axis=0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bee2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = {'cwola': (cwola_max, cwola_med, cwola_min, cwola_mean, cwola_std, cwola_smean, cwola_sstd, {'label':'CWoLa', 'color':'red'}),\n",
    "         #'dvsim': (dvsim_max, dvsim_med, dvsim_min, dvsim_mean, dvsim_std, dvsim_smean, dvsim_sstd, {'label':'Data vs. Sim', 'color':'blue', 'ls':':'}),\n",
    "        #'ocwola': (ocwola_max, ocwola_med, ocwola_min, ocwola_mean, ocwola_std, ocwola_smean, ocwola_sstd, {'label':'Optimal CWoLa', 'color':'red'}),\n",
    "          #'autoencoder': (ae_max, ae_med, ae_min, ae_mean, ae_std, ae_smean, ae_sstd, {'label':\"Autoencoder\".format(1), 'color':'blue'}),\n",
    "          #'hybrid': (hybrid_max, hybrid_med, hybrid_min, hybrid_mean, hybrid_std, hybrid_smean, hybrid_sstd, {'label':'hybrid', 'color':'tab:purple'}),\n",
    "          'ANN': (ann_max, ann_med, ann_min, ann_mean, ann_std, ann_smean, ann_sstd, {'label':'ANN', 'color':'tab:orange'}),\n",
    "          #'sacwola': (ksacwola_max, ksacwola_med, ksacwola_min, ksacwola_mean, ksacwola_std, ksacwola_smean, ksacwola_sstd, {'label':\"SA-CWoLa, $\\lambda = {}$\".format(myklambda), 'color':'black'}),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee83db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot without banding\n",
    "mylambda=.5\n",
    "myklambda = 1 #np.round(len(SR_background_data)/len(SR_background_sim), 3)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6)) \n",
    "gs = gridspec.GridSpec(1, 1, height_ratios=[1])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax0.yaxis.set_ticks_position('both')\n",
    "ax0.xaxis.set_ticks_position('both')\n",
    "ax0.tick_params(direction=\"in\",which=\"both\")\n",
    "ax0.minorticks_on()\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim([1,5*1e4])\n",
    "ERR=False\n",
    "#plt.plot(tpr_sup,1./fpr_sup,label=\"Supervised\",color='green')\n",
    "\n",
    "for band in bands:\n",
    "    if band in ['old-sacwola', 'hl-sacwola']:\n",
    "        continue\n",
    "    mu,std = bands[band][3:5]\n",
    "\n",
    "    plt.plot(tpr_pts, mu, **bands[band][-1])\n",
    "    if ERR:\n",
    "        plt.fill_between(tpr_pts, mu - std, mu + std, alpha=.2, color=bands[band][-1]['color'])\n",
    "#plt.plot(tpr_sup,1./tpr_sup,color=\"black\",ls=\":\",label=\"Random\")\n",
    "\n",
    "plt.xlabel(r'Signal Efficiency (True Positive Rate)',fontsize=20)\n",
    "plt.ylabel(r\"Rejection (1/False Positive Rate)\",fontsize=20)\n",
    "plt.legend(frameon=False,fontsize=15,loc=\"upper right\")\n",
    "\n",
    "# plt.title(\"Correlated LHCO Example\\n(Median of {} models)\".format(n_injections),fontsize=20,)\n",
    "#plt.savefig('C:/Users/knukl/Desktop/FINAL/plots/combined_corr_deltamj_extracorr.pdf', pdf=True, bbox_inches='tight')\n",
    "#plt.savefig('C:/Users/knukl/Desktop/FINAL/plots/combined_corr_deltamj_extracorr.png', pdf=False, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38028921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot without banding\n",
    "mylambda=.5\n",
    "myklambda = 1 #np.round(len(SR_background_data)/len(SR_background_sim), 3)\n",
    "\n",
    "ERR = False\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6)) \n",
    "gs = gridspec.GridSpec(1, 1, height_ratios=[1])\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax0.yaxis.set_ticks_position('both')\n",
    "ax0.xaxis.set_ticks_position('both')\n",
    "ax0.tick_params(direction=\"in\",which=\"both\")\n",
    "ax0.minorticks_on()\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.ylim([0, 6])\n",
    "\n",
    "\n",
    "\n",
    "for band in bands:\n",
    "    if band in ['old-sacwola', 'hl-sacwola']:\n",
    "        continue\n",
    "    mu,std = bands[band][5:7]\n",
    "    plt.plot(tpr_pts, tpr_pts*mu, **bands[band][-1])\n",
    "#plt.plot(tpr_sup,tpr_sup/np.sqrt(tpr_sup),color=\"black\",ls=\":\",label=\"Random\")\n",
    "\n",
    "plt.xlabel(r'Signal Efficiency (True Positive Rate)',fontsize=20)\n",
    "plt.ylabel(r\"Significance improvement\",fontsize=20)\n",
    "plt.legend(frameon=False,fontsize=10,loc=\"upper right\")\n",
    "#plt.title(r'$LHC$ $Olympics$ $Example$', loc='right', fontsize=20)\n",
    "\n",
    "#plt.savefig('C:/Users/knukl/Desktop/FINAL/plots/combined_relsigs_corr_deltamj_extracorr.pdf', pdf=True, bbox_inches='tight')\n",
    "#plt.savefig('C:/Users/knukl/Desktop/FINAL/plots/combined_relsigs_corr_deltamj_extracorr.png', pdf=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc5e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
